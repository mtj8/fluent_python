{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef866485",
   "metadata": {},
   "source": [
    "# Chapter 4: Unicode Text Versus Bytes\n",
    "\n",
    "In this chapter, we will visit the following topics:\n",
    "‚Ä¢ Characters, code points, and byte representations\n",
    "‚Ä¢ Unique features of binary sequences: bytes, bytearray, and memoryview\n",
    "‚Ä¢ Encodings for full Unicode and legacy character sets\n",
    "‚Ä¢ Avoiding and dealing with encoding errors\n",
    "‚Ä¢ Best practices when handling text files\n",
    "‚Ä¢ The default encoding trap and standard I/O issues\n",
    "‚Ä¢ Safe Unicode text comparisons with normalization\n",
    "‚Ä¢ Utility functions for normalization, case folding, and brute-force diacritic\n",
    "removal\n",
    "‚Ä¢ Proper sorting of Unicode text with locale and the pyuca library\n",
    "‚Ä¢ Character metadata in the Unicode database\n",
    "‚Ä¢ Dual-mode APIs that handle str and bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f051cff2",
   "metadata": {},
   "source": [
    "## Unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8275379d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, b'caf\\xc3\\xa9', 5, 'caf√©')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In Unicode, characters are represented with code points\n",
    "\n",
    "s = 'caf√©'\n",
    "b = s.encode('utf-8')\n",
    "len(s), b, len(b), b.decode('utf-8') # √© is two bytes in unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69fe8f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b'caf\\xc3\\xa9', bytearray(b'caf\\xc3\\xa9'), 99, b'c', bytearray(b'\\xa9'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bytes and bytearrays - bytes are between 0 (0x00) and 255 (0xFF)\n",
    "cafe = bytes('caf√©', encoding='utf_8') \n",
    "cafe_arr = bytearray(cafe)\n",
    "\n",
    "cafe, cafe_arr, cafe[0], cafe[:1], cafe_arr[-1:]\n",
    "\n",
    "# bytes and bytearrays support string methods that don't relate to formatting and Unicode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e33a0601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latin_1\tb'El Ni\\xf1o'\n",
      "utf_8\tb'El Ni\\xc3\\xb1o'\n",
      "utf_16\tb'\\xff\\xfeE\\x00l\\x00 \\x00N\\x00i\\x00\\xf1\\x00o\\x00'\n"
     ]
    }
   ],
   "source": [
    "# there are over a 100 codecs (encode/decode) in Python for text to byte / byte to text conversion\n",
    "\n",
    "for codec in ('latin_1', 'utf_8', 'utf_16'):\n",
    "    print(codec, 'El Ni√±o'.encode(codec), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf0f6054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b'S\\xc3\\xa3o Paulo',\n",
       " b'\\xff\\xfeS\\x00\\xe3\\x00o\\x00 \\x00P\\x00a\\x00u\\x00l\\x00o\\x00',\n",
       " b'S\\xe3o Paulo')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Error handling\n",
    "\n",
    "# UnicodeEncodeError\n",
    "city = 'S√£o Paulo'\n",
    "\n",
    "city.encode('utf_8'), city.encode('utf_16'), city.encode('iso8859_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f86313",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\xe3' in position 1: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcity\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcp437\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/encodings/cp437.py:12\u001b[0m, in \u001b[0;36mCodec.encode\u001b[0;34m(self, input, errors)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m,\u001b[38;5;28minput\u001b[39m,errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharmap_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoding_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode character '\\xe3' in position 1: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "city.encode('cp437')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d821cb8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b'So Paulo', b'S?o Paulo', b'S&#227;o Paulo')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city.encode('cp437', errors=\"ignore\"), city.encode('cp437', errors=\"replace\"), city.encode('cp437', errors=\"xmlcharrefreplace\")\n",
    "# encoding should always work if your characters are purely ASCII"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53deb10",
   "metadata": {},
   "source": [
    "## Coping with Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a71c181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Montr√©al', 'MontrŒπal', 'Montr–òal')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UnicodeDecodeError\n",
    "\n",
    "octets = b'Montr\\xe9al' \n",
    "\n",
    "octets.decode('cp1252'), octets.decode('iso8859_7'), octets.decode('koi8_r') # the last two can decode, but since the bytes map to something else\n",
    "# in that codec, it outputs something random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fda52517",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xe9 in position 5: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43moctets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xe9 in position 5: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "octets.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e49ed525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MontrÔøΩal'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "octets.decode('utf-8', errors=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a82b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can't exactly determine the encoding of some given bytes, but you can use the chardet library to determine it heuristically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d9b7b2",
   "metadata": {},
   "source": [
    "### Handling Text Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49b9a9ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<_io.TextIOWrapper name='cafe.txt' mode='w' encoding='utf-8'>, 4, None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handling text files\n",
    "\n",
    "# Usually, you want to decode from bytes to str as early as possible, then encode back to bytes as late as possible.\n",
    "# This is known as the Unicode sandwich\n",
    "\n",
    "# Check out this error that occurs on Windows\n",
    "fp = open('cafe.txt', 'w', encoding='utf-8')\n",
    "fp, fp.write('caf√©'), fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d6925d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.stat('cafe.txt').st_size # 5 since utf-8 encodes √© as 2 bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e15e05d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<_io.TextIOWrapper name='cafe.txt' mode='r' encoding='cp1252'>, 'caf√É¬©')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp2 = open('cafe.txt', encoding='cp1252') # on Windows, cp1252 is the default, so this explicit encoding wouldn't be needed\n",
    "fp2, fp2.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb469f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'caf√©'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp3 = open('cafe.txt', encoding='utf-8') # on Mac, utf-8 is the default, so this explicit wouldn't be needed.\n",
    "# you should always pass an explicit encoding to get rid of these bugs\n",
    "\n",
    "fp3.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98563807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<_io.BufferedReader name='cafe.txt'>, b'caf\\xc3\\xa9')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp4 = open('cafe.txt', 'rb')\n",
    "fp4, fp4.read() # bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da00ec05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " locale.getpreferredencoding() -> 'UTF-8'\n",
      "                 type(my_file) -> <class '_io.TextIOWrapper'>\n",
      "              my_file.encoding -> 'UTF-8'\n",
      "           sys.stdout.isatty() -> False\n",
      "           sys.stdout.encoding -> 'UTF-8'\n",
      "            sys.stdin.isatty() -> False\n",
      "            sys.stdin.encoding -> 'utf-8'\n",
      "           sys.stderr.isatty() -> False\n",
      "           sys.stderr.encoding -> 'UTF-8'\n",
      "      sys.getdefaultencoding() -> 'utf-8'\n",
      "   sys.getfilesystemencoding() -> 'utf-8'\n"
     ]
    }
   ],
   "source": [
    "import locale\n",
    "import sys\n",
    "expressions = \"\"\"\n",
    " locale.getpreferredencoding()\n",
    " type(my_file)\n",
    " my_file.encoding\n",
    " sys.stdout.isatty()\n",
    " sys.stdout.encoding\n",
    " sys.stdin.isatty()\n",
    " sys.stdin.encoding\n",
    " sys.stderr.isatty()\n",
    " sys.stderr.encoding\n",
    " sys.getdefaultencoding()\n",
    " sys.getfilesystemencoding()\n",
    " \"\"\"\n",
    "\n",
    "my_file = open('cafe.txt', 'w')\n",
    "\n",
    "for exp in expressions.split():\n",
    "    value = eval(exp)\n",
    "    print(f'{exp:>30} -> {value!r}') # on Windows, the locale and my_file.encoding would be cp1252."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a7c99c",
   "metadata": {},
   "source": [
    "## Normalizing Unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d125ce02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('caf√©', 'cafeÃÅ', 4, 5, False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalizing unicode \n",
    "\n",
    "s1 = 'caf√©'\n",
    "s2 = 'cafe\\N{COMBINING ACUTE ACCENT}'\n",
    "\n",
    "# s1 and s2 are not the same because of the way they're constructed, even though they look the same\n",
    "s1, s2, len(s1), len(s2), s1 == s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f38a8e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unicodedata import normalize\n",
    "\n",
    "# NFD decomposes, NFC combines\n",
    "# by default, keyboards will make composed characters, so they should be NFC\n",
    "normalize('NFD', s1) == normalize('NFD', s2), normalize('NFC', s1) == normalize('NFC', s2) # NFC combines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e8c9d0cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('OHM SIGN', 'GREEK CAPITAL LETTER OMEGA', False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unicodedata import name\n",
    "\n",
    "ohm = '\\u2126'\n",
    "ohm_c = normalize('NFC', ohm)\n",
    "name(ohm), name(ohm_c), ohm == ohm_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "708658e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('¬Ω', '1‚ÅÑ2')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NFKC and NFKD are stricter normalizations, with K standing for \"compatibility\"\n",
    "\n",
    "half = '\\N{VULGAR FRACTION ONE HALF}'\n",
    "half, normalize('NFKC', half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b6d334de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tDIGIT ONE\n",
      "‚ÅÑ\tFRACTION SLASH\n",
      "2\tDIGIT TWO\n"
     ]
    }
   ],
   "source": [
    "for char in normalize('NFKC', half):\n",
    "    print(char, name(char), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1cd9d105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('LATIN SMALL LETTER SHARP S', '√ü', 'ss')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# case folding\n",
    "# almost the same as .lower() but with some special cases.\n",
    "\n",
    "eszett = '√ü'\n",
    "eszett_cf = eszett.casefold()\n",
    "name(eszett), eszett, eszett_cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "949f4e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# when working with text in many languages, you can use\n",
    "# functions like these to compare text\n",
    "\n",
    "def nfc_equal(str1, str2):\n",
    "    return normalize('NFC', str1) == normalize('NFC', str2)\n",
    "\n",
    "def fold_equal(str1, str2):\n",
    "    return (normalize('NFC', str1).casefold() ==\n",
    "        normalize('NFC', str2).casefold())\n",
    "\n",
    "s3 = 'Stra√üe'\n",
    "s4 = 'strasse'\n",
    "\n",
    "nfc_equal(s3, s4), fold_equal(s3, s4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec42951a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‚ÄúHerr Vo√ü: ‚Ä¢ ¬Ω cup of ≈ítker‚Ñ¢ caffe latte ‚Ä¢ bowl of acai.‚Äù'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hack is to remove diacritics (accents, cedillas)\n",
    "# this changes the meaning of the word but can help with user-facing stuff\n",
    "# like google search, since realistically, users aren't going to use\n",
    "# accents much\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "def remove_marks(txt):\n",
    "    norm_txt = unicodedata.normalize('NFD', txt)\n",
    "    shaved = ''.join(c for c in norm_txt\n",
    "                     if not unicodedata.combining(c)) # filter out combining marks\n",
    "    return unicodedata.normalize('NFC', shaved)\n",
    "\n",
    "order = '‚ÄúHerr Vo√ü: ‚Ä¢ ¬Ω cup of ≈ítker‚Ñ¢ caff√® latte ‚Ä¢ bowl of a√ßa√≠.‚Äù'\n",
    "remove_marks(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eacd7a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Œñ\n",
      "['Œñ']\n",
      "False Œµ\n",
      "['Œñ', 'Œµ']\n",
      "False ÃÅ\n",
      "['Œñ', 'Œµ', 'ÃÅ']\n",
      "False œÜ\n",
      "['Œñ', 'Œµ', 'ÃÅ', 'œÜ']\n",
      "False œÖ\n",
      "['Œñ', 'Œµ', 'ÃÅ', 'œÜ', 'œÖ']\n",
      "False œÅ\n",
      "['Œñ', 'Œµ', 'ÃÅ', 'œÜ', 'œÖ', 'œÅ']\n",
      "False Œø\n",
      "['Œñ', 'Œµ', 'ÃÅ', 'œÜ', 'œÖ', 'œÅ', 'Œø']\n",
      "False œÇ\n",
      "['Œñ', 'Œµ', 'ÃÅ', 'œÜ', 'œÖ', 'œÅ', 'Œø', 'œÇ']\n",
      "False ,\n",
      "['Œñ', 'Œµ', 'ÃÅ', 'œÜ', 'œÖ', 'œÅ', 'Œø', 'œÇ', ',']\n",
      "False  \n",
      "['Œñ', 'Œµ', 'ÃÅ', 'œÜ', 'œÖ', 'œÅ', 'Œø', 'œÇ', ',', ' ']\n",
      "False Z\n",
      "['Œñ', 'Œµ', 'ÃÅ', 'œÜ', 'œÖ', 'œÅ', 'Œø', 'œÇ', ',', ' ', 'Z']\n",
      "True e\n",
      "['Œñ', 'Œµ', 'ÃÅ', 'œÜ', 'œÖ', 'œÅ', 'Œø', 'œÇ', ',', ' ', 'Z', 'e']\n",
      "True ÃÅ\n",
      "True f\n",
      "['Œñ', 'Œµ', 'ÃÅ', 'œÜ', 'œÖ', 'œÅ', 'Œø', 'œÇ', ',', ' ', 'Z', 'e', 'f']\n",
      "True i\n",
      "['Œñ', 'Œµ', 'ÃÅ', 'œÜ', 'œÖ', 'œÅ', 'Œø', 'œÇ', ',', ' ', 'Z', 'e', 'f', 'i']\n",
      "True r\n",
      "['Œñ', 'Œµ', 'ÃÅ', 'œÜ', 'œÖ', 'œÅ', 'Œø', 'œÇ', ',', ' ', 'Z', 'e', 'f', 'i', 'r']\n",
      "True o\n",
      "['Œñ', 'Œµ', 'ÃÅ', 'œÜ', 'œÖ', 'œÅ', 'Œø', 'œÇ', ',', ' ', 'Z', 'e', 'f', 'i', 'r', 'o']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('ŒñŒµœÜœÖœÅŒøœÇ, Zefiro', 'ŒñŒ≠œÜœÖœÅŒøœÇ, Zefiro')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a function that might make more sense is to remove attached marks\n",
    "# only if the base character is from the Latin alphabet\n",
    "\n",
    "def remove_marks_latin(txt):\n",
    "    norm_txt = unicodedata.normalize('NFD', txt)\n",
    "    latin_base = False\n",
    "\n",
    "    preserve = []\n",
    "    for c in norm_txt:\n",
    "        print(latin_base, c)\n",
    "        if unicodedata.combining(c) and latin_base:\n",
    "            continue # skip diacritic on latin base char\n",
    "        preserve.append(c)\n",
    "        print(preserve)\n",
    "        \n",
    "        # if not combining char, it's a new base char\n",
    "        if not unicodedata.combining(c):\n",
    "            latin_base = c in string.ascii_letters\n",
    "    \n",
    "    shaved=''.join(preserve)\n",
    "\n",
    "    return unicodedata.normalize('NFC', shaved)\n",
    "\n",
    "greek = 'ŒñŒ≠œÜœÖœÅŒøœÇ, Z√©firo'\n",
    "\n",
    "remove_marks(greek), remove_marks_latin(greek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "13580408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can go even more extreme with translations\n",
    "single_map = str.maketrans(\"\"\"‚Äö∆í‚ÄûÀÜ‚Äπ‚Äò‚Äô‚Äú‚Äù‚Ä¢‚Äì‚ÄîÀú‚Ä∫\"\"\",\n",
    " \"\"\"'f\"^<''\"\"---~>\"\"\")\n",
    "\n",
    "multi_map = str.maketrans({\n",
    " '‚Ç¨': 'EUR',\n",
    " '‚Ä¶': '...',\n",
    " '√Ü': 'AE',\n",
    " '√¶': 'ae',\n",
    " '≈í': 'OE',\n",
    " '≈ì': 'oe',\n",
    " '‚Ñ¢': '(TM)',\n",
    " '‚Ä∞': '<per mille>',\n",
    " '‚Ä†': '**',\n",
    " '‚Ä°': '***',\n",
    "})\n",
    "\n",
    "multi_map.update(single_map) # merge tables\n",
    "\n",
    "def dewinize(txt: str):\n",
    "    return txt.translate(multi_map)\n",
    "\n",
    "def asciize(txt):\n",
    "    no_marks = remove_marks_latin(dewinize(txt))\n",
    "    no_marks = no_marks.replace('√ü', 'ss')\n",
    "\n",
    "    return unicodedata.normalize('NFKC', no_marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d3f20b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"Herr Vo√ü: - ¬Ω cup of OEtker(TM) caff√® latte - bowl of a√ßa√≠.\"',\n",
       " '\"Herr Voss: - 1‚ÅÑ2 cup of OEtker(TM) caffe latte - bowl of acai.\"')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dewinize(order), asciize(order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395f8a5c",
   "metadata": {},
   "source": [
    "### Sorting Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeea6a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acerola', 'atemoia', 'a√ßa√≠', 'caju', 'caj√°']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can sort text by comparing code points, but this doesn't work for\n",
    "# non-ASCII characters\n",
    "\n",
    "fruits = ['caju', 'atemoia', 'caj√°', 'a√ßa√≠', 'acerola']\n",
    "sorted(fruits) # acai should come first, and caja comes before caju"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7b3c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('pt_BR.UTF-8', ['a√ßa√≠', 'acerola', 'atemoia', 'caj√°', 'caju'])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can use locale.strxfrm instead:\n",
    "import locale\n",
    "\n",
    "my_locale = locale.setlocale(locale.LC_COLLATE, 'pt_BR.UTF-8')\n",
    "my_locale, sorted(fruits, key=locale.strxfrm)\n",
    "# setlocale is a global setting, so don't do this in a library\n",
    "# if the locale isn't in your OS, it will raise a locale.Error\n",
    "# in short, YMMV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7c0ce3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyuca is a pure-Python implementation of the \n",
    "# Unicode Collation Algorithm (UCA)\n",
    "import pyuca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9456e208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a√ßa√≠', 'acerola', 'atemoia', 'caj√°', 'caju']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coll = pyuca.Collator()\n",
    "fruits = ['caju', 'atemoia', 'caj√°', 'a√ßa√≠', 'acerola']\n",
    "sorted(fruits, key=coll.sort_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7bc55a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('LATIN CAPITAL LETTER A', 'SMILING FACE WITH HEART-SHAPED EYES')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the Unicode \n",
    "from unicodedata import name\n",
    "\n",
    "name('A'), name('üòç')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f3b693",
   "metadata": {},
   "source": [
    "### Example: Unicode character finder utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b69370b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U+1F638\tüò∏\tGRINNING CAT FACE WITH SMILING EYES\n",
      "U+1F63A\tüò∫\tSMILING CAT FACE WITH OPEN MOUTH\n",
      "U+1F63B\tüòª\tSMILING CAT FACE WITH HEART-SHAPED EYES\n",
      "U+1700\t·úÄ\tTAGALOG LETTER A\n",
      "U+1701\t·úÅ\tTAGALOG LETTER I\n",
      "U+1702\t·úÇ\tTAGALOG LETTER U\n",
      "U+1703\t·úÉ\tTAGALOG LETTER KA\n",
      "U+1704\t·úÑ\tTAGALOG LETTER GA\n",
      "U+1705\t·úÖ\tTAGALOG LETTER NGA\n",
      "U+1706\t·úÜ\tTAGALOG LETTER TA\n",
      "U+1707\t·úá\tTAGALOG LETTER DA\n",
      "U+1708\t·úà\tTAGALOG LETTER NA\n",
      "U+1709\t·úâ\tTAGALOG LETTER PA\n",
      "U+170A\t·úä\tTAGALOG LETTER BA\n",
      "U+170B\t·úã\tTAGALOG LETTER MA\n",
      "U+170C\t·úå\tTAGALOG LETTER YA\n",
      "U+170D\t·úç\tTAGALOG LETTER RA\n",
      "U+170E\t·úé\tTAGALOG LETTER LA\n",
      "U+170F\t·úè\tTAGALOG LETTER WA\n",
      "U+1710\t·úê\tTAGALOG LETTER SA\n",
      "U+1711\t·úë\tTAGALOG LETTER HA\n",
      "U+1712\t·úí\tTAGALOG VOWEL SIGN I\n",
      "U+1713\t·úì\tTAGALOG VOWEL SIGN U\n",
      "U+1714\t·úî\tTAGALOG SIGN VIRAMA\n",
      "U+1715\t·úï\tTAGALOG SIGN PAMUDPOD\n",
      "U+171F\t·úü\tTAGALOG LETTER ARCHAIC RA\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# character finder utility\n",
    "import sys, unicodedata\n",
    "\n",
    "START, END = ord(' '), sys.maxunicode + 1\n",
    "\n",
    "def find(*query_words, start=START, end=END):\n",
    "    res = []\n",
    "    query = {w.upper() for w in query_words} # set comp\n",
    "    for code in range(start, end):\n",
    "        char = chr(code)\n",
    "        name = unicodedata.name(char, None)\n",
    "        if name and query.issubset(name.split()):\n",
    "            print(f'U+{code:04X}\\t{char}\\t{name}')\n",
    "            # res.append(f'U+{code:04X}\\t{char}\\t{name}')\n",
    "\n",
    "    # return res\n",
    "\n",
    "def search(words):\n",
    "    if words:\n",
    "        res = find(*words)\n",
    "        # return res\n",
    "    else:\n",
    "        print(\"Please give words\")\n",
    "\n",
    "search(['smiling', 'cat']), search(['tagalog'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87845b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U+0031\t  1   \tre_dig\tisdig\tisnum\t 1.00\tDIGIT ONE\n",
      "U+00bc\t  ¬º   \t-\t-\tisnum\t 0.25\tVULGAR FRACTION ONE QUARTER\n",
      "U+00b2\t  ¬≤   \t-\tisdig\tisnum\t 2.00\tSUPERSCRIPT TWO\n",
      "U+0969\t  ‡•©   \tre_dig\tisdig\tisnum\t 3.00\tDEVANAGARI DIGIT THREE\n",
      "U+136b\t  ·ç´   \t-\tisdig\tisnum\t 3.00\tETHIOPIC DIGIT THREE\n",
      "U+216b\t  ‚Ö´   \t-\t-\tisnum\t12.00\tROMAN NUMERAL TWELVE\n",
      "U+2466\t  ‚ë¶   \t-\tisdig\tisnum\t 7.00\tCIRCLED DIGIT SEVEN\n",
      "U+2480\t  ‚íÄ   \t-\t-\tisnum\t13.00\tPARENTHESIZED NUMBER THIRTEEN\n",
      "U+3285\t  „äÖ   \t-\t-\tisnum\t 6.00\tCIRCLED IDEOGRAPH SIX\n"
     ]
    }
   ],
   "source": [
    "# numerical character metadata\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "re_digit = re.compile(r'\\d')\n",
    "\n",
    "sample = '1\\xbc\\xb2\\u0969\\u136b\\u216b\\u2466\\u2480\\u3285'\n",
    "\n",
    "for char in sample:\n",
    "    print(f'U+{ord(char):04x}',\n",
    "        char.center(6),\n",
    "        're_dig' if re_digit.match(char) else '-',\n",
    "        'isdig' if char.isdigit() else '-',\n",
    "        'isnum' if char.isnumeric() else '-',\n",
    "        f'{unicodedata.numeric(char):5.2f}',\n",
    "        unicodedata.name(char),\n",
    "        sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15c19f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['cafe.txt', '4.ipynb'], [b'cafe.txt', b'4.ipynb'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some functions in the standard library accept both str and bytes\n",
    "# and behave differently for both\n",
    "\n",
    "# examples include regular expressions (re) and os\n",
    "import os\n",
    "os.listdir('.'), os.listdir(b'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5a2a2b",
   "metadata": {},
   "source": [
    "To conclude:\n",
    "\n",
    "1 character does not mean 1 byte.\n",
    "Encoding and decoding is tricky. Watch out for defaults.\n",
    "Normalization is needed for text matching.\n",
    "You can do a lot of stuff with the Unicode database, like the utililty\n",
    "to search for characters by name in 28 lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80f4b51",
   "metadata": {},
   "source": [
    "### Soapbox\n",
    "\n",
    "Mojibake (gibberish text)\n",
    "\n",
    "Storing str code points in RAM is flexible. Cool implementation details etc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
